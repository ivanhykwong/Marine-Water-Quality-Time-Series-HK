{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load libraries for image processing steps\n",
    "# pip install --user sentinel2tools-master.zip\n",
    "# pip install --user landsatxplore-master.zip\n",
    "from landsatxplore.api import API\n",
    "from landsatxplore.earthexplorer import EarthExplorer\n",
    "from sentinel2download.overlap import Sentinel2Overlap\n",
    "from sentinel2download.downloader import Sentinel2Downloader\n",
    "import os\n",
    "import shutil\n",
    "from datetime import date, datetime, timedelta\n",
    "import tarfile \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from simpledbf import Dbf5\n",
    "import requests\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "arcpy.CheckOutExtension(\"spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and download Landsat satellite images\n",
    "def downloadlandsat(startdate, enddate):\n",
    "    # Initialize a new API instance and get an access key\n",
    "    username = \"username\"  # change your EarthExplorer username and password\n",
    "    password = \"password\"\n",
    "    api = API(username, password)\n",
    "    # 22.13,113.81,22.59,114.52\n",
    "    # https://github.com/yannforget/landsatxplore/blob/master/landsatxplore/api.py\n",
    "    # Search for Landsat TM scenes\n",
    "    scenes = api.search(\n",
    "        dataset='landsat_ot_c2_l1', bbox=(113.81, 22.13, 114.52, 22.59),\n",
    "        start_date=startdate,  # start_date='2014-01-01',\n",
    "        end_date=enddate,   # end_date='2015-12-31',\n",
    "        max_cloud_cover=20, max_results=1000\n",
    "    )\n",
    "    # print(f\"{len(scenes)} scenes found.\")\n",
    "    # Log out\n",
    "    api.logout()\n",
    "    # Downloading scenes\n",
    "    if len(scenes) > 0:\n",
    "        ee = EarthExplorer(username, password)\n",
    "        for s in scenes:\n",
    "            ee.download(s['entity_id'], output_dir='D:/WaterQuality/datadownload')\n",
    "        ee.logout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Sentinel overlap tiles\n",
    "aoi_path = \"D:/WaterQuality/aoi/aoi_geojson.json\"\n",
    "overlap = Sentinel2Overlap(aoi_path)\n",
    "tiles = overlap.overlap()\n",
    "print(f\"Overlapped tiles: {tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and download Sentinel satellite images from Google Cloud\n",
    "def downloadsentinel(startdate, enddate):\n",
    "    CONSTRAINTS = {'CLOUDY_PIXEL_PERCENTAGE': 20.0, }\n",
    "    loader = Sentinel2Downloader(api_key=\"D:/WaterQuality/xxx.json\")  # change your Google Cloud Sentinel API key\n",
    "    loaded = loader.download(product_type=\"L1C\", tiles=['49QGE','49QGF','49QHE','49QHF'], \n",
    "                            start_date=startdate, # \"2016-01-01\"\n",
    "                            end_date=enddate, # \"2016-01-05\"\n",
    "                            output_dir=\"D:/WaterQuality/datadownload\",\n",
    "                            cores=2, constraints=CONSTRAINTS, full_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract acolite_py_win_20221114.0.tar.gz\n",
    "# Save the following text as a txt file \"acolite/setting_landsat.txt\"\n",
    "## ACOLITE settings\n",
    "limit=22.13,113.81,22.59,114.52\n",
    "inputfile=D:/WaterQuality/datadownload/extract\n",
    "output=D:/WaterQuality/datadownload/atmocor\n",
    "dsf_interface_reflectance=False\n",
    "dsf_residual_glint_correction=True\n",
    "glint_mask_rhos_threshold=0.15\n",
    "l2w_parameters=None\n",
    "l2r_export_geotiff=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a single Landsat image\n",
    "def preprocessLandsat(tar):\n",
    "    # extract tar\n",
    "    datadir = 'D:/WaterQuality/datadownload'\n",
    "    os.chdir(datadir)\n",
    "    file = tarfile.open(tar)\n",
    "    file.extractall('extract')\n",
    "    file.close()\n",
    "    # run acolite\n",
    "    acolitepath = \"D:/WaterQuality/acolite/acolite_py_win/dist/acolite/acolite.exe\"\n",
    "    settingpath = \"D:/WaterQuality/acolite/setting_landsat.txt\"\n",
    "    os.system(acolitepath+\" --cli --settings=\"+settingpath)\n",
    "    def merge_and_mask():\n",
    "        # merge 7 bands\n",
    "        os.chdir('atmocor')\n",
    "        tiflist = glob.glob('*L2R_rhos_*.tif')\n",
    "        bandorder = [2, 3, 4, 7, 8, 0, 1]\n",
    "        tiflist = [tiflist[i] for i in bandorder]\n",
    "        env.workspace = 'D:/WaterQuality/datadownload/atmocor'\n",
    "        arcpy.CompositeBands_management(tiflist, \"compbands.tif\")\n",
    "        # mask land and cloud\n",
    "        ras = Raster(\"compbands.tif\")\n",
    "        qaband = Raster(glob.glob(datadir+'/extract/*QA_PIXEL.TIF')[0])\n",
    "        qaband_m = SetNull(qaband>22200,1)\n",
    "        qaband_m = FocalStatistics(qaband_m, NbrCircle(3,\"CELL\"), \"MEAN\", \"NODATA\") # expand radius 3\n",
    "        ras_m = ExtractByMask(ras, qaband_m)\n",
    "        swir = Raster(\"compbands.tif\\Band_6\")\n",
    "        green = Raster(\"compbands.tif\\Band_3\")\n",
    "        nir = Raster(\"compbands.tif\\Band_5\")\n",
    "        red = Raster(\"compbands.tif\\Band_4\")\n",
    "        ndvi1 = arcpy.sa.Float((red-nir)/(red+nir))\n",
    "        ndvi1_m = SetNull(ndvi1<0,1)\n",
    "        ndwi = arcpy.sa.Float((green-swir)/(green+swir))\n",
    "        ndwi_m = SetNull(ndwi<0,1)\n",
    "        swir_m = SetNull(swir>0.15,1)\n",
    "        ras_m = ExtractByMask(ras_m, ndvi1_m)\n",
    "        ras_m = ExtractByMask(ras_m, ndwi_m)\n",
    "        ras_m = ExtractByMask(ras_m, swir_m)\n",
    "        # reproject\n",
    "        aoi = \"D:/WaterQuality/aoi/aoi.shp\"\n",
    "        outfilename = \"D:/WaterQuality/reflectance/\"+tar.replace(\".tar\",\".tif\")\n",
    "        arcpy.management.ProjectRaster(ras_m, \"compbands_p.tif\", aoi)            \n",
    "        arcpy.management.Clip(\"compbands_p.tif\", aoi, \"compbands_p_c.tif\",                                \n",
    "                            \"#\", \"#\", \"NONE\",\"MAINTAIN_EXTENT\")\n",
    "        arcpy.management.Resample(\"compbands_p_c.tif\", outfilename, 0.00028571429)\n",
    "    merge_and_mask()\n",
    "    # empty extract and atmocor\n",
    "    def emptyfolder(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)    \n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "    emptyfolder(\"D:/WaterQuality/datadownload/extract\")\n",
    "    emptyfolder(\"D:/WaterQuality/datadownload/atmocor\")\n",
    "    # delete tarfile\n",
    "    os.chdir(datadir)\n",
    "    os.unlink(tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess all Landsat images\n",
    "def preprocessLandsat_all():\n",
    "    datadir = 'D:/WaterQuality/datadownload'\n",
    "    os.chdir(datadir)\n",
    "    tarlist = glob.glob('*.tar')\n",
    "    if len(tarlist)>0:\n",
    "        for tar in tarlist:\n",
    "            preprocessLandsat(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the following text as a txt file \"acolite/setting_sentinel.txt\"\n",
    "## ACOLITE settings\n",
    "limit=22.13,113.81,22.59,114.52\n",
    "inputfile=\n",
    "output=D:/WaterQuality/datadownload/atmocor\n",
    "s2_target_res=20\n",
    "dsf_interface_reflectance=False\n",
    "dsf_residual_glint_correction=True\n",
    "glint_mask_rhos_threshold=0.15\n",
    "l2w_parameters=None\n",
    "l2r_export_geotiff=True\n",
    "\n",
    "# Save the following text as a txt file \"acolite/setting_sentinel2.txt\"\n",
    "## ACOLITE settings\n",
    "limit=22.13,113.81,22.59,114.52\n",
    "inputfile=D:/WaterQuality/datadownload\\S2A_MSIL1C_20240717T025551_N0510_R032_T49QHE_20240717T053551.SAFE\n",
    "output=D:/WaterQuality_LandsatSentinel/datadownload/atmocor\n",
    "s2_target_res=20\n",
    "dsf_interface_reflectance=False\n",
    "dsf_residual_glint_correction=True\n",
    "glint_mask_rhos_threshold=0.15\n",
    "l2w_parameters=None\n",
    "l2r_export_geotiff=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a single Sentinel image\n",
    "def preprocessSentinel(safefolder):\n",
    "    datadir = 'D:/WaterQuality/datadownload'\n",
    "    os.chdir(datadir)\n",
    "    # run acolite\n",
    "    settingtemp = \"D:/WaterQuality/acolite/setting_sentinel.txt\"\n",
    "    settingpath = \"D:/WaterQuality/acolite/setting_sentinel2.txt\"\n",
    "    # Read in the file\n",
    "    with open(settingtemp, 'r') as file:\n",
    "        filedata = file.read()\n",
    "        filedata = filedata.replace('inputfile=', 'inputfile='+os.path.join(datadir,safefolder))\n",
    "    # Write the file out again\n",
    "    with open(settingpath, 'w') as file:\n",
    "        file.write(filedata)\n",
    "    acolitepath = \"D:/WaterQuality/acolite/acolite_py_win/dist/acolite/acolite.exe\"\n",
    "    os.system(acolitepath+\" --cli --settings=\"+settingpath)\n",
    "    def merge_and_mask():\n",
    "        # merge 7 bands\n",
    "        os.chdir('atmocor')\n",
    "        tiflist = glob.glob('*L2R_rhos_*.tif')\n",
    "        if len(tiflist)==0: # if acolite does not produce any files\n",
    "            return\n",
    "        bandorder = [2, 3, 4, 5, 10, 0, 1]\n",
    "        tiflist = [tiflist[i] for i in bandorder]\n",
    "        env.workspace = 'D:/WaterQuality/datadownload/atmocor'\n",
    "        arcpy.CompositeBands_management(tiflist, \"compbands.tif\")\n",
    "        arcpy.management.Resample(\"compbands.tif\", \"compbands_r.tif\", 30)\n",
    "        # mask land and cloud\n",
    "        ras = Raster(\"compbands_r.tif\")\n",
    "        swir = Raster(\"compbands_r.tif\\Band_6\")\n",
    "        green = Raster(\"compbands_r.tif\\Band_3\")\n",
    "        nir = Raster(\"compbands_r.tif\\Band_5\")\n",
    "        red = Raster(\"compbands_r.tif\\Band_4\")\n",
    "        cloud_m = SetNull((red>0.2)&(nir>0.2),1)\n",
    "        cloud_m = FocalStatistics(cloud_m, NbrCircle(3,\"CELL\"), \"MEAN\", \"NODATA\") # expand radius 3\n",
    "        ndvi1 = arcpy.sa.Float((red-nir)/(red+nir))\n",
    "        ndvi1_m = SetNull(ndvi1<0,1)\n",
    "        ndwi2 = arcpy.sa.Float((green-swir)/(green+swir))\n",
    "        ndwi2_m = SetNull(ndwi2<0,1)\n",
    "        swir_m = SetNull(swir>0.15,1)\n",
    "        nir_m = SetNull((nir>0.03)&(red>0.08)&(ndwi2_m==1)&(swir_m==1)&(cloud_m==1),1) # remaining haze\n",
    "        nir_m = FocalStatistics(nir_m, NbrCircle(1,\"CELL\"), \"MEAN\", \"NODATA\") # expand radius 1\n",
    "        ras_m = ExtractByMask(ras, cloud_m)\n",
    "        ras_m = ExtractByMask(ras_m, ndvi1_m)\n",
    "        ras_m = ExtractByMask(ras_m, ndwi2_m)\n",
    "        ras_m = ExtractByMask(ras_m, swir_m)\n",
    "        ras_m = ExtractByMask(ras_m, nir_m)\n",
    "        # reproject\n",
    "        aoi = \"D:/WaterQuality/aoi/aoi.shp\"\n",
    "        outfilename = \"D:/WaterQuality/reflectance/\"+safefolder.replace(\".SAFE\",\".tif\")\n",
    "        arcpy.management.ProjectRaster(ras_m, \"compbands_p.tif\", aoi)\n",
    "        arcpy.management.Clip(\"compbands_p.tif\", aoi, \"compbands_p_c.tif\",                                \n",
    "                            \"#\", \"#\", \"NONE\",\"MAINTAIN_EXTENT\")\n",
    "        arcpy.management.Resample(\"compbands_p_c.tif\", outfilename, 0.00028571429)\n",
    "    merge_and_mask()\n",
    "    # empty extract and atmocor\n",
    "    def emptyfolder(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)    \n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "    emptyfolder(\"D:/WaterQuality/datadownload/atmocor\")\n",
    "    # delete whole safefolder\n",
    "    os.chdir(datadir)\n",
    "    shutil.rmtree(safefolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess all Sentinel images\n",
    "def preprocessSentinel_all():\n",
    "    datadir = 'D:/WaterQuality/datadownload'\n",
    "    os.chdir(datadir)\n",
    "    safelist = glob.glob('*.safe')\n",
    "    if len(safelist)>0:\n",
    "        for safefolder in safelist:\n",
    "            preprocessSentinel(safefolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get dates in each month\n",
    "from datetime import date, datetime, timedelta\n",
    "def monthstart(year, month):\n",
    "    first_date = datetime(year, month, 1)\n",
    "    return first_date.strftime(\"%Y-%m-%d\")\n",
    "def monthmid1(year, month):\n",
    "    mid_date = datetime(year, month, 15)\n",
    "    return mid_date.strftime(\"%Y-%m-%d\")\n",
    "def monthmid2(year, month):\n",
    "    mid_date = datetime(year, month, 16)\n",
    "    return mid_date.strftime(\"%Y-%m-%d\")\n",
    "def monthend(year, month):\n",
    "    if month == 12:\n",
    "        last_date = datetime(year, month, 31)\n",
    "    else:\n",
    "        last_date = datetime(year, month + 1, 1) + timedelta(days=-1)\n",
    "    return last_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the functions to download and preprocess all Landsat and Sentinel imagery\n",
    "for year in [2020,2021,2022]:\n",
    "    for month in range(1,13):\n",
    "        downloadlandsat(monthstart(year, month), monthend(year, month))\n",
    "        preprocessLandsat_all()\n",
    "        downloadsentinel(monthstart(year, month), monthend(year, month))\n",
    "        preprocessSentinel_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Tier 2 Landsat imagery\n",
    "def removeLandsatT2():\n",
    "    os.chdir(\"D:/WaterQuality/reflectance\")\n",
    "    Tier2list = glob.glob('LC*T2.*')\n",
    "    if len(Tier2list)>0:\n",
    "        for T2file in Tier2list:\n",
    "            os.unlink(T2file)\n",
    "# removeLandsatT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all Landsat imagery\n",
    "def renameLandsat_all():\n",
    "    os.chdir(\"D:/WaterQuality/reflectance\")\n",
    "    Landsatlist = glob.glob('LC*')\n",
    "    for Landsatfile in Landsatlist:\n",
    "        nfilename = Landsatfile[0:25]+Landsatfile[40:] # first 25 characters & from 40 to end\n",
    "        os.rename(Landsatfile, nfilename)\n",
    "# renameLandsat_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all Sentinel imagery\n",
    "def renameSentinel_all():\n",
    "    os.chdir(\"D:/WaterQuality/reflectance\")\n",
    "    Sentinellist = glob.glob('S2*')\n",
    "    for Sentinelfile in Sentinellist:\n",
    "        nfilename = Sentinelfile[0:19]+Sentinelfile[37:44]+Sentinelfile[60:]\n",
    "        if os.path.isfile(nfilename) == True:\n",
    "            nfilename = Sentinelfile[0:19]+Sentinelfile[37:44]+'a'+Sentinelfile[60:]\n",
    "        os.rename(Sentinelfile, nfilename)\n",
    "# renameSentinel_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaic tiles acquired on the same day\n",
    "def mosaictiles(): \n",
    "    os.chdir(\"D:/WaterQuality/reflectance\")\n",
    "    env.workspace = \"D:/WaterQuality/reflectance\"\n",
    "    Landsatlist = glob.glob('LC*')\n",
    "    Landsatdatelist = [i[17:25] for i in Landsatlist]\n",
    "    Sentinellist = glob.glob('S2*')\n",
    "    Sentineldatelist = [i[11:19] for i in Sentinellist]\n",
    "    datelist = sorted(list(set(Landsatdatelist+Sentineldatelist))) # get unique date\n",
    "    imglist = glob.glob('*.tif')\n",
    "    for d in datelist:\n",
    "        img_match = [img for img in imglist if d in img]\n",
    "        outfolder = \"D:/WaterQuality/preprocess_finish\"\n",
    "        outfilename = \"LandsatSentinel_\"+d+\".tif\"\n",
    "        if len(img_match)==1:\n",
    "            arcpy.management.CopyRaster(img_match[0], os.path.join(outfolder, outfilename))\n",
    "        if len(img_match)>1:\n",
    "            arcpy.MosaicToNewRaster_management(img_match,outfolder,outfilename,\"\",\"32_BIT_FLOAT\",\"\",\"7\",\"MEAN\",\"\")\n",
    "# mosaictiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove images that cover too few valid pixels\n",
    "def deleteimage_lowvalid():\n",
    "    os.chdir(\"D:/WaterQuality/preprocess_finish\")\n",
    "    env.workspace = \"D:/WaterQuality/preprocess_finish\"\n",
    "    imglist = glob.glob('*.tif')\n",
    "    for img in imglist:\n",
    "        ras_np = arcpy.RasterToNumPyArray(img,\"\",\"\",\"\",-9999)[0]\n",
    "        if (ras_np != -9999).sum() < (2390000*0.2): # largest valid count = 2390000\n",
    "            arcpy.management.Delete(img)\n",
    "# deleteimage_lowvalid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pixel values based on the monitoring station locations\n",
    "def extractpixel(imgname):\n",
    "    os.chdir(\"D:/WaterQuality/preprocess_finish\")\n",
    "    env.workspace = \"D:/WaterQuality/preprocess_finish\"\n",
    "    imgdate = imgname[16:24]\n",
    "    station_shp = \"D:/WaterQuality/stationdata/MonitoringStation_wgs84_76.shp\"\n",
    "    extract_dbf = \"D:/WaterQuality/extract/extract.dbf\"\n",
    "    arcpy.sa.Sample(imgname, station_shp, extract_dbf, \"BILINEAR\", \"FID\")\n",
    "    extract_df = Dbf5(extract_dbf).to_dataframe()\n",
    "    extract_df[\"monitoring\"] = range(0,len(extract_df))\n",
    "    arcpy.management.Delete(extract_dbf)\n",
    "    extract_df.columns = ['Monitoring','X','Y','B1','B2','B3','B4','B5','B6','B7','monitoring']\n",
    "    station_df = Dbf5(\"D:/WaterQuality/stationdata/MonitoringStation_wgs84_76.dbf\").to_dataframe()\n",
    "    station_df[\"monitoring\"] = range(0,len(station_df))\n",
    "    station_df = station_df[[\"monitoring\",\"WaterStati\"]]\n",
    "    extract_df = extract_df.merge(station_df, on=\"monitoring\")\n",
    "    extract_df = extract_df.drop(columns=['Monitoring','X','Y','monitoring'])\n",
    "    extract_df = extract_df[extract_df[\"B1\"] > -1] # remove no data (-9999) rows\n",
    "    extract_df[\"Date\"] = imgdate\n",
    "    return extract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save the pixel values as a csv file\n",
    "os.chdir(\"D:/WaterQuality/preprocess_finish\")\n",
    "imglist = glob.glob('*.tif')\n",
    "dflist = [extractpixel(img) for img in imglist]\n",
    "extract_df_all = pd.concat(dflist)\n",
    "extract_df_all.to_csv(\"D:/WaterQuality/extract/extract_df_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine station data different years\n",
    "os.chdir(\"D:/WaterQuality/stationdata\")\n",
    "csvlist = glob.glob('marine-historical-*.csv')\n",
    "csvlist = [pd.read_csv(c) for c in csvlist]\n",
    "df = pd.concat(csvlist)\n",
    "df.to_csv(\"marine-historical-2010-2022.csv\")\n",
    "# Additional manual step: subset only surface water and replace <0.2 to 0.1, <0.5 to 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge image pixel value and station data that are same day\n",
    "stationdata_df = pd.read_csv(\"D:/WaterQuality/stationdata/marine-historical-2010-2022.csv\")\n",
    "extract_df_all = pd.read_csv(\"D:/WaterQuality/extract/extract_df_all.csv\", index_col=0)\n",
    "stationdata_df = stationdata_df[[\"Station\",\"Dates\",\"Chlorophyll-a (µg/L)\",\"Suspended Solids (mg/L)\",\"Turbidity (NTU)\"]]\n",
    "stationdata_df.columns = [\"WaterStati\", \"Date\", \"Chla\", \"SS\", \"Tur\"]\n",
    "stationdata_df[\"Date\"] = pd.to_datetime(stationdata_df[\"Date\"], format='%d/%m/%Y').dt.strftime('%Y%m%d').astype(int)\n",
    "img_stationdata_merge = stationdata_df.merge(extract_df_all, on=[\"WaterStati\",\"Date\"])\n",
    "img_stationdata_merge.to_csv(\"D:/WaterQuality/extract/img_stationdata_merge.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create independent variables\n",
    "df = pd.read_csv(\"D:/WaterQuality/extract/img_stationdata_merge.csv\", index_col=0)\n",
    "bands = ['B' + str(b) for b in [*range(1,8)]]\n",
    "wl = [443,490,560,660,865,1610,2195]  #wavelength in nm\n",
    "\n",
    "# Multiply 10\n",
    "for i in bands:\n",
    "  df[i] = df[i]*10\n",
    "# Square and cubic\n",
    "for i in bands:\n",
    "  df[i+'_2'] = df[i]**2\n",
    "  df[i+'_3'] = df[i]**3\n",
    "# Two-band ratio\n",
    "for i in bands:\n",
    "  for j in bands:\n",
    "    if (i != j) & (i < j):\n",
    "      df['NR_'+i+j] = ((df[i] - df[j]) / (df[i] + df[j])).clip(lower=-1.0, upper=1.0)\n",
    "# Three-band ratio\n",
    "for i in range(0,7):\n",
    "  for j in range(0,7):\n",
    "    for k in range(0,7):\n",
    "      if (j == i+1) & (k == j+1):\n",
    "        df['TB_'+bands[i]+bands[j]+bands[k]] = (((1/df[bands[i]])-(1/df[bands[j]]))*df[bands[k]]).clip(lower=-1.0, upper=1.0)\n",
    "# Line height algorithm\n",
    "for i in range(0,7):\n",
    "  for j in range(0,7):\n",
    "    for k in range(0,7):\n",
    "      if (j == i+1) & (k == j+1):\n",
    "        df['LH_'+bands[i]+bands[j]+bands[k]] = df[bands[j]] - df[bands[i]] - ((df[bands[k]] - df[bands[i]]) * ((wl[j]-wl[i])/(wl[k]-wl[i])))\n",
    "\n",
    "df.to_csv('D:/WaterQuality/extract/df_variable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/WaterQuality/extract/df_variable.csv\", index_col=0)\n",
    "df = df.drop(columns = [\"WaterStati\",\"Date\"])\n",
    "df_var = df.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load libraries for neural network steps\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stepwise variable selection\n",
    "# Modified from https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn/24447#24447\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out = 0.1, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - pandas.DataFrame with the target column\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    y = y.to_numpy()\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype='float64')\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.index[new_pval.argmin()]\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.index[pvalues.argmax()]\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain model performance based on cross validation\n",
    "wq = ['Chla','SS','Tur']\n",
    "def model_cv(wq_name):\n",
    "    y = df[wq_name]\n",
    "    step_var = stepwise_selection(df_var, y, verbose=False)\n",
    "    X = df[step_var]\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    df_cv = pd.DataFrame()\n",
    "    for train, test in kfold.split(X, y):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(6, activation='sigmoid'),\n",
    "            tf.keras.layers.Dense(3, activation='sigmoid'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam())\n",
    "        model.fit(X.iloc[train,], y[train], epochs=2000)\n",
    "        y_predict = model.predict(X.iloc[test,]).flatten()\n",
    "        y_test = y[test]\n",
    "        df_cv1 = pd.DataFrame({\"y_predict\":y_predict, \"y_test\":y_test})\n",
    "        df_cv = pd.concat([df_cv,df_cv1])\n",
    "    corr_test = np.corrcoef(df_cv[\"y_test\"], df_cv[\"y_predict\"])[0, 1]\n",
    "    rmse_test = mean_squared_error(df_cv[\"y_test\"], df_cv[\"y_predict\"], squared=False)\n",
    "    mae_test = mean_absolute_error(df_cv[\"y_test\"], df_cv[\"y_predict\"])\n",
    "    model_cv_df = pd.DataFrame({'WQ': [wq_name], 'var':[step_var], 'corr_test': [corr_test], \n",
    "                                'rmse_test': [rmse_test], 'mae_test': [mae_test]})\n",
    "    return(model_cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain model performance\n",
    "model_cv_Chla = model_cv('Chla')\n",
    "model_cv_SS = model_cv('SS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chla model\n",
    "wq_name = \"Chla\"\n",
    "y = df[wq_name]\n",
    "step_var = stepwise_selection(df_var, y, verbose=False)\n",
    "X = df[step_var]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(6, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam())\n",
    "model.fit(X, y, epochs=2000)\n",
    "model.save(\"D:/WaterQuality/extract/Chla_model.keras\")\n",
    "\n",
    "# SS model\n",
    "wq_name = \"SS\"\n",
    "y = df[wq_name]\n",
    "step_var = stepwise_selection(df_var, y, verbose=False)\n",
    "X = df[step_var]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(6, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam())\n",
    "model.fit(X, y, epochs=2000)\n",
    "model.save(\"D:/WaterQuality/extract/SS_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model weights from the trained models\n",
    "Chla_model = tf.keras.models.load_model(\"D:/WaterQuality/extract/Chla_model.keras\")\n",
    "pd.Series(Chla_model.get_weights()).to_json(\"D:/WaterQuality/extract/Chla_modelweight.json\")\n",
    "SS_model = tf.keras.models.load_model(\"D:/WaterQuality/extract/SS_model.keras\")\n",
    "pd.Series(SS_model.get_weights()).to_json(\"D:/WaterQuality/extract/SS_modelweight.json\")\n",
    "# OR pd.read_json(\"D:/WaterQuality/extract/Chla_modelweight.json\", typ=\"series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to predict all imagery\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "arcpy.CheckOutExtension(\"spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict Chla\n",
    "def predictChla(imgname):\n",
    "        outfolder = \"D:/WaterQuality/predict\"\n",
    "        os.chdir(outfolder)\n",
    "        env.workspace = outfolder\n",
    "        # ANN layers: 14, 6, 3, 1\n",
    "        p = pd.read_json(\"D:/WaterQuality/extract/Chla_modelweight.json\", typ=\"series\")\n",
    "        # Chla ['NR_B2B4', 'NR_B2B6', 'NR_B3B6', 'NR_B3B4', 'TB_B1B2B3', \n",
    "        # 'TB_B4B5B6', 'B2_3', 'B5', 'B4_2', 'B3_3', 'TB_B2B3B4', 'B6_3', 'NR_B1B6', 'B6_2']\n",
    "        outname = imgname.replace(\"preprocess_finish\", \"predict\").replace(\"LandsatSentinel_20\", \"Chla_20\")\n",
    "        b1 = Raster(imgname+\"/Band_1\")*10\n",
    "        b2 = Raster(imgname+\"/Band_2\")*10\n",
    "        b3 = Raster(imgname+\"/Band_3\")*10\n",
    "        b4 = Raster(imgname+\"/Band_4\")*10\n",
    "        b5 = Raster(imgname+\"/Band_5\")*10\n",
    "        b6 = Raster(imgname+\"/Band_6\")*10\n",
    "        ras_0 = b1*0\n",
    "        ras_neg1 = ras_0 - 1\n",
    "        ras_1 = ras_0 + 1\n",
    "        v1 = CellStatistics([CellStatistics([(b2-b4)/(b2+b4),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v2 = CellStatistics([CellStatistics([(b2-b6)/(b2+b6),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v3 = CellStatistics([CellStatistics([(b3-b6)/(b3+b6),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v4 = CellStatistics([CellStatistics([(b3-b4)/(b3+b4),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v5 = CellStatistics([CellStatistics([(((1/b1)-(1/b2))*b3),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v6 = CellStatistics([CellStatistics([(((1/b4)-(1/b5))*b6),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v7 = b2 ** 3\n",
    "        v8 = b5\n",
    "        v9 = b4 ** 2\n",
    "        v10 = b3 ** 3\n",
    "        v11 = CellStatistics([CellStatistics([(((1/b2)-(1/b3))*b4),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v12 = b6 ** 3\n",
    "        v13 = CellStatistics([CellStatistics([(b1-b6)/(b1+b6),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v14 = b6 ** 2\n",
    "\n",
    "        h1n1 = (Exp((p[1][0]+v1*p[0][0][0]+v2*p[0][1][0]+v3*p[0][2][0]+v4*p[0][3][0]+v5*p[0][4][0]+\n",
    "                v6*p[0][5][0]+v7*p[0][6][0]+v8*p[0][7][0]+v9*p[0][8][0]+v10*p[0][9][0]+\n",
    "                v11*p[0][10][0]+v12*p[0][11][0]+v13*p[0][12][0]+v14*p[0][13][0])*(-1))+1)**-1\n",
    "        h1n2 = (Exp((p[1][1]+v1*p[0][0][1]+v2*p[0][1][1]+v3*p[0][2][1]+v4*p[0][3][1]+v5*p[0][4][1]+\n",
    "                v6*p[0][5][1]+v7*p[0][6][1]+v8*p[0][7][1]+v9*p[0][8][1]+v10*p[0][9][1]+\n",
    "                v11*p[0][10][1]+v12*p[0][11][1]+v13*p[0][12][1]+v14*p[0][13][1])*(-1))+1)**-1\n",
    "        h1n3 = (Exp((p[1][2]+v1*p[0][0][2]+v2*p[0][1][2]+v3*p[0][2][2]+v4*p[0][3][2]+v5*p[0][4][2]+\n",
    "                v6*p[0][5][2]+v7*p[0][6][2]+v8*p[0][7][2]+v9*p[0][8][2]+v10*p[0][9][2]+\n",
    "                v11*p[0][10][2]+v12*p[0][11][2]+v13*p[0][12][2]+v14*p[0][13][2])*(-1))+1)**-1\n",
    "        h1n4 = (Exp((p[1][3]+v1*p[0][0][3]+v2*p[0][1][3]+v3*p[0][2][3]+v4*p[0][3][3]+v5*p[0][4][3]+\n",
    "                v6*p[0][5][3]+v7*p[0][6][3]+v8*p[0][7][3]+v9*p[0][8][3]+v10*p[0][9][3]+\n",
    "                v11*p[0][10][3]+v12*p[0][11][3]+v13*p[0][12][3]+v14*p[0][13][3])*(-1))+1)**-1\n",
    "        h1n5 = (Exp((p[1][4]+v1*p[0][0][4]+v2*p[0][1][4]+v3*p[0][2][4]+v4*p[0][3][4]+v5*p[0][4][4]+\n",
    "                v6*p[0][5][4]+v7*p[0][6][4]+v8*p[0][7][4]+v9*p[0][8][4]+v10*p[0][9][4]+\n",
    "                v11*p[0][10][4]+v12*p[0][11][4]+v13*p[0][12][4]+v14*p[0][13][4])*(-1))+1)**-1\n",
    "        h1n6 = (Exp((p[1][5]+v1*p[0][0][5]+v2*p[0][1][5]+v3*p[0][2][5]+v4*p[0][3][5]+v5*p[0][4][5]+\n",
    "                v6*p[0][5][5]+v7*p[0][6][5]+v8*p[0][7][5]+v9*p[0][8][5]+v10*p[0][9][5]+\n",
    "                v11*p[0][10][5]+v12*p[0][11][5]+v13*p[0][12][5]+v14*p[0][13][5])*(-1))+1)**-1\n",
    "\n",
    "        h2n1 = (Exp((p[3][0]+h1n1*p[2][0][0]+h1n2*p[2][1][0]+h1n3*p[2][2][0]+\n",
    "                h1n4*p[2][3][0]+h1n5*p[2][4][0]+h1n6*p[2][5][0])*(-1))+1)**-1\n",
    "        h2n2 = (Exp((p[3][1]+h1n1*p[2][0][1]+h1n2*p[2][1][1]+h1n3*p[2][2][1]+\n",
    "                h1n4*p[2][3][1]+h1n5*p[2][4][1]+h1n6*p[2][5][1])*(-1))+1)**-1\n",
    "        h2n3 = (Exp((p[3][2]+h1n1*p[2][0][2]+h1n2*p[2][1][2]+h1n3*p[2][2][2]+\n",
    "                h1n4*p[2][3][2]+h1n5*p[2][4][2]+h1n6*p[2][5][2])*(-1))+1)**-1\n",
    "\n",
    "        pred = CellStatistics([p[5][0]+h2n1*p[4][0][0]+h2n2*p[4][1][0]+h2n3*p[4][2][0],ras_0], \"MAXIMUM\")\n",
    "        arcpy.management.CopyRaster(pred, outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict SS\n",
    "def predictSS(imgname):\n",
    "        outfolder = \"D:/WaterQuality/predict\"\n",
    "        os.chdir(outfolder)\n",
    "        env.workspace = outfolder\n",
    "        # ANN layers: 9, 6, 3, 1\n",
    "        p = pd.read_json(\"D:/WaterQuality/extract/SS_modelweight.json\", typ=\"series\")\n",
    "        # SS ['TB_B2B3B4', 'LH_B4B5B6', 'B3_3', 'B4_2', 'LH_B5B6B7', 'TB_B3B4B5', 'NR_B5B6', 'NR_B1B4', 'B2_3']\n",
    "        outname = imgname.replace(\"preprocess_finish\", \"predict\").replace(\"LandsatSentinel_20\", \"SuSo_20\")\n",
    "        b1 = Raster(imgname+\"/Band_1\")*10\n",
    "        b2 = Raster(imgname+\"/Band_2\")*10\n",
    "        b3 = Raster(imgname+\"/Band_3\")*10\n",
    "        b4 = Raster(imgname+\"/Band_4\")*10\n",
    "        b5 = Raster(imgname+\"/Band_5\")*10\n",
    "        b6 = Raster(imgname+\"/Band_6\")*10\n",
    "        b7 = Raster(imgname+\"/Band_7\")*10\n",
    "        ras_0 = b1*0\n",
    "        ras_neg1 = ras_0 - 1\n",
    "        ras_1 = ras_0 + 1\n",
    "        v1 = CellStatistics([CellStatistics([(((1/b2)-(1/b3))*b4),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v2 = b5-b4-((b6-b4)*((865-660)/(1610-660)))\n",
    "        v3 = b3 ** 3\n",
    "        v4 = b4 ** 2\n",
    "        v5 = b6-b5-((b7-b5)*((1610-865)/(2195-865)))\n",
    "        v6 = CellStatistics([CellStatistics([(((1/b3)-(1/b4))*b5),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v7 = CellStatistics([CellStatistics([(b5-b6)/(b5+b6),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v8 = CellStatistics([CellStatistics([(b1-b4)/(b1+b4),ras_neg1], \"MAXIMUM\"),ras_1],\"MINIMUM\")\n",
    "        v9 = b2 ** 3\n",
    "\n",
    "        h1n1 = (Exp((p[1][0]+v1*p[0][0][0]+v2*p[0][1][0]+v3*p[0][2][0]+v4*p[0][3][0]+v5*p[0][4][0]+\n",
    "                v6*p[0][5][0]+v7*p[0][6][0]+v8*p[0][7][0]+v9*p[0][8][0])*(-1))+1)**-1\n",
    "        h1n2 = (Exp((p[1][1]+v1*p[0][0][1]+v2*p[0][1][1]+v3*p[0][2][1]+v4*p[0][3][1]+v5*p[0][4][1]+\n",
    "                v6*p[0][5][1]+v7*p[0][6][1]+v8*p[0][7][1]+v9*p[0][8][1])*(-1))+1)**-1\n",
    "        h1n3 = (Exp((p[1][2]+v1*p[0][0][2]+v2*p[0][1][2]+v3*p[0][2][2]+v4*p[0][3][2]+v5*p[0][4][2]+\n",
    "                v6*p[0][5][2]+v7*p[0][6][2]+v8*p[0][7][2]+v9*p[0][8][2])*(-1))+1)**-1\n",
    "        h1n4 = (Exp((p[1][3]+v1*p[0][0][3]+v2*p[0][1][3]+v3*p[0][2][3]+v4*p[0][3][3]+v5*p[0][4][3]+\n",
    "                v6*p[0][5][3]+v7*p[0][6][3]+v8*p[0][7][3]+v9*p[0][8][3])*(-1))+1)**-1\n",
    "        h1n5 = (Exp((p[1][4]+v1*p[0][0][4]+v2*p[0][1][4]+v3*p[0][2][4]+v4*p[0][3][4]+v5*p[0][4][4]+\n",
    "                v6*p[0][5][4]+v7*p[0][6][4]+v8*p[0][7][4]+v9*p[0][8][4])*(-1))+1)**-1\n",
    "        h1n6 = (Exp((p[1][5]+v1*p[0][0][5]+v2*p[0][1][5]+v3*p[0][2][5]+v4*p[0][3][5]+v5*p[0][4][5]+\n",
    "                v6*p[0][5][5]+v7*p[0][6][5]+v8*p[0][7][5]+v9*p[0][8][5])*(-1))+1)**-1\n",
    "\n",
    "        h2n1 = (Exp((p[3][0]+h1n1*p[2][0][0]+h1n2*p[2][1][0]+h1n3*p[2][2][0]+\n",
    "                h1n4*p[2][3][0]+h1n5*p[2][4][0]+h1n6*p[2][5][0])*(-1))+1)**-1\n",
    "        h2n2 = (Exp((p[3][1]+h1n1*p[2][0][1]+h1n2*p[2][1][1]+h1n3*p[2][2][1]+\n",
    "                h1n4*p[2][3][1]+h1n5*p[2][4][1]+h1n6*p[2][5][1])*(-1))+1)**-1\n",
    "        h2n3 = (Exp((p[3][2]+h1n1*p[2][0][2]+h1n2*p[2][1][2]+h1n3*p[2][2][2]+\n",
    "                h1n4*p[2][3][2]+h1n5*p[2][4][2]+h1n6*p[2][5][2])*(-1))+1)**-1\n",
    "\n",
    "        pred = CellStatistics([p[5][0]+h2n1*p[4][0][0]+h2n2*p[4][1][0]+h2n3*p[4][2][0],ras_0], \"MAXIMUM\")\n",
    "        arcpy.management.CopyRaster(pred, outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to all imagery\n",
    "imglist = glob.glob('D:/WaterQuality/preprocess_finish/*.tif')\n",
    "for i in imglist:\n",
    "    predictChla(i)\n",
    "    predictSS(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chla and SS raster for the latest date\n",
    "aoi_water = \"D:/WaterQuality/aoi/aoi_water.shp\"\n",
    "# Chla\n",
    "chlalist = glob.glob(\"D:/WaterQuality/predict/Chla_*.tif\")\n",
    "latestimg = chlalist[len(chlalist)-1]\n",
    "latestimg = latestimg[len(latestimg)-17:len(latestimg)]\n",
    "mergeras = arcpy.management.MosaicToNewRaster(chlalist, \"D:/WaterQuality/predict_display\", \"merge_\"+latestimg, \"\", \"32_BIT_FLOAT\", \"\", 1, \"LAST\")\n",
    "mergeras_focal = FocalStatistics(mergeras, NbrRectangle(3,3,\"CELL\"), \"MEDIAN\")\n",
    "outname = (\"D:/WaterQuality/predict_display/merge_\"+latestimg).replace(\".tif\", \"_smoothclip.tif\")\n",
    "arcpy.management.Clip(mergeras_focal, \"\", outname, aoi_water, \"\", \"ClippingGeometry\")\n",
    "# SS\n",
    "sslist = glob.glob(\"D:/WaterQuality/predict/SuSo_*.tif\")\n",
    "latestimg = sslist[len(sslist)-1]\n",
    "latestimg = latestimg[len(latestimg)-17:len(latestimg)]\n",
    "mergeras = arcpy.management.MosaicToNewRaster(sslist, \"D:/WaterQuality/predict_display\", \"merge_\"+latestimg, \"\", \"32_BIT_FLOAT\", \"\", 1, \"LAST\")\n",
    "mergeras_focal = FocalStatistics(mergeras, NbrRectangle(3,3,\"CELL\"), \"MEDIAN\")\n",
    "outname = (\"D:/WaterQuality/predict_display/merge_\"+latestimg).replace(\".tif\", \"_smoothclip.tif\")\n",
    "arcpy.management.Clip(mergeras_focal, \"\", outname, aoi_water, \"\", \"ClippingGeometry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datapoints using Fishnet tool in ArcGIS\n",
    "# Datapoints are used to plot charts in Dashboard\n",
    "# Extract and append values for the first date\n",
    "datapoint = \"D:/WaterQuality/ArcGISPro/DataPoint.shp\"\n",
    "datapoint_d1 = \"D:/WaterQuality/ArcGISPro/DataPoint_d1.shp\"\n",
    "datapoint_d2 = \"D:/WaterQuality/ArcGISPro/DataPoint_d2.shp\"\n",
    "datapoint_all = \"D:/WaterQuality/ArcGISPro/DataPoint_all.shp\"\n",
    "arcpy.management.Copy(datapoint, datapoint_d1)\n",
    "arcpy.management.Copy(datapoint, datapoint_d2)\n",
    "chla_d1 = \"D:/WaterQuality/predict/Chla_20130708.tif\"\n",
    "ss_d1 = \"D:/WaterQuality/predict/SuSo_20130708.tif\"\n",
    "arcpy.sa.ExtractMultiValuesToPoints(datapoint_d1, chla_d1+\" value\", \"BILINEAR\")\n",
    "arcpy.sa.ExtractMultiValuesToPoints(datapoint_d2, ss_d1+\" value\", \"BILINEAR\")\n",
    "with arcpy.da.UpdateCursor(datapoint_d1, [\"value\",\"Date\",\"parameter\",\"latest\",\"DateRange\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] < 0:\n",
    "            cursor.deleteRow()\n",
    "        else:\n",
    "            row[1] = 20130708\n",
    "            row[2] = \"Chla\"\n",
    "            row[3] = 1\n",
    "            row[4] = \"Day\"\n",
    "            cursor.updateRow(row)\n",
    "with arcpy.da.UpdateCursor(datapoint_d2, [\"value\",\"Date\",\"parameter\",\"latest\",\"DateRange\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] < 0:\n",
    "            cursor.deleteRow()\n",
    "        else:\n",
    "            row[1] = 20130708\n",
    "            row[2] = \"SS\"\n",
    "            row[3] = 1\n",
    "            row[4] = \"Day\"\n",
    "            cursor.updateRow(row)\n",
    "arcpy.management.Copy(datapoint_d1, datapoint_all)\n",
    "arcpy.management.Append(datapoint_d2, datapoint_all)\n",
    "with arcpy.da.UpdateCursor(datapoint_d1, [\"Date\",\"DateRange\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[0] = round(20130708/100)*100+15\n",
    "        row[1] = \"Month\"\n",
    "        cursor.updateRow(row)\n",
    "with arcpy.da.UpdateCursor(datapoint_d2, [\"Date\",\"DateRange\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[0] = round(20130708/100)*100+15\n",
    "        row[1] = \"Month\"\n",
    "        cursor.updateRow(row)\n",
    "arcpy.management.Append(datapoint_d1, datapoint_all)\n",
    "arcpy.management.Append(datapoint_d2, datapoint_all)\n",
    "arcpy.management.Delete(datapoint_d1)\n",
    "arcpy.management.Delete(datapoint_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append datapoint from 2nd to latest dates\n",
    "datapoint = \"D:/WaterQuality/ArcGISPro/DataPoint.shp\"\n",
    "datapoint_d1 = \"D:/WaterQuality/ArcGISPro/DataPoint_d1.shp\"\n",
    "datapoint_d2 = \"D:/WaterQuality/ArcGISPro/DataPoint_d2.shp\"\n",
    "datapoint_all = \"D:/WaterQuality/ArcGISPro/DataPoint_all.shp\"\n",
    "chlalist = glob.glob(\"D:/WaterQuality/predict/Chla_*.tif\")\n",
    "sslist = glob.glob(\"D:/WaterQuality/predict/SuSo_*.tif\")\n",
    "for i in range(1,len(chlalist)):\n",
    "    chla_d1 = chlalist[i]\n",
    "    ss_d1 = sslist[i]\n",
    "    arcpy.management.Copy(datapoint, datapoint_d1)\n",
    "    arcpy.management.Copy(datapoint, datapoint_d2)\n",
    "    arcpy.sa.ExtractMultiValuesToPoints(datapoint_d1, chla_d1+\" value\", \"BILINEAR\")\n",
    "    arcpy.sa.ExtractMultiValuesToPoints(datapoint_d2, ss_d1+\" value\", \"BILINEAR\")\n",
    "    # remove points with no data, else add date and extract list of valid points\n",
    "    newdatapt = []\n",
    "    d1 = int(chla_d1[len(chla_d1)-12:len(chla_d1)-4])\n",
    "    d1_month = round(d1/100)*100+15\n",
    "    with arcpy.da.UpdateCursor(datapoint_d1, [\"value\",\"pt\",\"Date\",\"parameter\",\"latest\",\"DateRange\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] < 0:\n",
    "                cursor.deleteRow()\n",
    "            else:\n",
    "                newdatapt.append(row[1])\n",
    "                row[2] = d1 # Date\n",
    "                row[3] = \"Chla\"\n",
    "                row[4] = 1 # latest\n",
    "                row[5] = \"Day\"\n",
    "                cursor.updateRow(row)\n",
    "    # modify values for SS\n",
    "    with arcpy.da.UpdateCursor(datapoint_d2, [\"value\",\"pt\",\"Date\",\"parameter\",\"latest\",\"DateRange\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] < 0:\n",
    "                cursor.deleteRow()\n",
    "            else:\n",
    "                row[2] = d1 # Date\n",
    "                row[3] = \"SS\"\n",
    "                row[4] = 1 # latest\n",
    "                row[5] = \"Day\"\n",
    "                cursor.updateRow(row)    \n",
    "    # modify latest in datapoint_all\n",
    "    with arcpy.da.UpdateCursor(datapoint_all, [\"pt\",\"latest\",\"Date\",\"DateRange\"], \"latest = 1\") as cursor: # where clause\n",
    "        for row in cursor:\n",
    "            if row[0] in newdatapt: # if latest image provides obs on this pt\n",
    "                row[1] = 0\n",
    "                if row[3]==\"Month\" and row[2]==d1_month: # if same month as latest image\n",
    "                    row[1] = 1\n",
    "                cursor.updateRow(row)\n",
    "    arcpy.management.Append(datapoint_d1, datapoint_all)\n",
    "    arcpy.management.Append(datapoint_d2, datapoint_all)\n",
    "    # add monthly average data\n",
    "    with arcpy.da.UpdateCursor(datapoint_d1, [\"Date\",\"DateRange\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[0] = d1_month\n",
    "            row[1] = \"Month\"\n",
    "            cursor.updateRow(row)\n",
    "    with arcpy.da.UpdateCursor(datapoint_d2, [\"Date\",\"DateRange\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[0] = d1_month\n",
    "            row[1] = \"Month\"\n",
    "            cursor.updateRow(row)\n",
    "    arcpy.management.Append(datapoint_d1, datapoint_all)\n",
    "    arcpy.management.Append(datapoint_d2, datapoint_all)\n",
    "    arcpy.management.Delete(datapoint_d1)\n",
    "    arcpy.management.Delete(datapoint_d2)\n",
    "    print(\"Finish: \"+ str(i)+\"/\"+str(len(chlalist)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
